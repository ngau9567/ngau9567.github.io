{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVMKoban",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0reB_IxM8tS"
      },
      "source": [
        "Practice homework:\n",
        "\n",
        "Spam dataset: \n",
        "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
        "\n",
        "### Features: \n",
        "* Number of words\n",
        "* \"bag of words\" --\n",
        "** Split text into words, remove punctuation, lower case ✓\n",
        "** Create a matrix where every word is a column ✓\n",
        "** Values: word count in a message or 0 ✓\n",
        "\n",
        "** Think about feature selection -- using coorrelation or P-value or chi-squarred or??? ✓\n",
        "\n",
        "** Train and test an SVM; do a few iterations to refine feature selection ✓"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ACZWG_kCK0"
      },
      "source": [
        "#0. Import the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy7xuFIuMt4J",
        "outputId": "a24ace45-15d9-49db-faf0-378dd75ba3a9"
      },
      "source": [
        "# Mount data drive\n",
        "from google.colab import drive\n",
        "drive.mount('/data/')\n",
        "data_dir = '/data/My Drive/EMSE 6575/SVM Homework'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /data/; to attempt to forcibly remount, call drive.mount(\"/data/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "gqFHudrQNiFV",
        "outputId": "0fa36cac-70fc-4d5f-917a-b922f5c5b0af"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_excel(data_dir + \"/spam_resaved.xlsx\")\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['category', 'text']\n",
        "df['doc_id'] = df.index\n",
        "df['doc_id'] = df['doc_id'].apply(lambda x: \"doc_\" + str(x))\n",
        "print(df['category'].value_counts())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham     4825\n",
            "spam     747\n",
            "Name: category, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>doc_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
              "      <td>doc_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>doc_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
              "      <td>doc_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>doc_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>doc_4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  category  ... doc_id\n",
              "0      ham  ...  doc_0\n",
              "1      ham  ...  doc_1\n",
              "2     spam  ...  doc_2\n",
              "3      ham  ...  doc_3\n",
              "4      ham  ...  doc_4\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDK2usL-kHWE"
      },
      "source": [
        "#1. Tokenize and clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAy-F2GsZtnC",
        "outputId": "97d0c4bb-8e07-4ae0-bc80-933b0c309645"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcDwgnd1TC0I",
        "outputId": "27210519-f5f7-4fa1-f253-4d8023d8e8d6"
      },
      "source": [
        "# Minimal example\n",
        "print(df['text'][0])\n",
        "tokens = str(df['text'][0]).split()\n",
        "tokens = [token.lower() for token in tokens]\n",
        "tokens = [re.sub(r'[^\\w\\s]','',token) for token in tokens]\n",
        "tokens = [token for token in tokens if len(token) >= 3]\n",
        "tokens = [token for token in tokens if not token in stopwords.words()]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jurong',\n",
              " 'point',\n",
              " 'crazy',\n",
              " 'available',\n",
              " 'bugis',\n",
              " 'great',\n",
              " 'world',\n",
              " 'buffet',\n",
              " 'got',\n",
              " 'amore']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "2U8VbEdcPis0",
        "outputId": "174b0e65-81b8-4d36-bc41-588987d41fe3"
      },
      "source": [
        "token_dfs = []\n",
        "for i in range(0, len(df)):\n",
        "  # split the text into tokens\n",
        "  tokens = str(df['text'][i]).split()\n",
        "  # drop case\n",
        "  tokens = [token.lower() for token in tokens]\n",
        "  \n",
        "  # create a tidy structure for the parsed and cleaned tokens\n",
        "  temp = pd.DataFrame({'token': tokens})\n",
        "  \n",
        "  # append  unique document id and text category\n",
        "  temp['doc_id'] = df['doc_id'][i]\n",
        "  temp['category'] = df['category'][i]\n",
        "\n",
        "  # collect the temp token dataframes\n",
        "  token_dfs.append(temp)\n",
        "\n",
        "token_df = pd.concat(token_dfs)\n",
        "print(\"Initial token count: \" + str(len(token_df)))\n",
        "\n",
        "#remove stop words\n",
        "token_df = token_df[token_df['token'].isin(stopwords.words()) == False]\n",
        "print(\"Drop stop words: \" + str(len(token_df)))\n",
        "\n",
        "# drop puncuation\n",
        "token_df['token'] = token_df['token'].apply(lambda x: re.sub(r'[^\\w\\s]','',x))  \n",
        "\n",
        "#remove rare words\n",
        "rare_words = token_df['token'].value_counts().reset_index()\n",
        "rare_words.columns = ['term', 'count']         \n",
        "rare_words = rare_words['term'][rare_words['count'] <= 4].tolist()\n",
        "token_df = token_df[token_df['token'].isin(rare_words) == False]\n",
        "print(\"Drop rare words: \" + str(len(token_df)))\n",
        "\n",
        "#remove short words\n",
        "token_df = token_df[token_df['token'].apply(len) >= 3]\n",
        "print(\"Drop short words: \" + str(len(token_df)))\n",
        "\n",
        "token_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial token count: 86335\n",
            "Drop stop words: 51730\n",
            "Drop rare words: 40001\n",
            "Drop short words: 33900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>doc_id</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>point</td>\n",
              "      <td>doc_0</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crazy</td>\n",
              "      <td>doc_0</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>available</td>\n",
              "      <td>doc_0</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bugis</td>\n",
              "      <td>doc_0</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>great</td>\n",
              "      <td>doc_0</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token doc_id category\n",
              "3       point  doc_0      ham\n",
              "4       crazy  doc_0      ham\n",
              "5   available  doc_0      ham\n",
              "8       bugis  doc_0      ham\n",
              "10      great  doc_0      ham"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVsssIUCkRac"
      },
      "source": [
        "# 2. Create a term frequency table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-firQg1nr4M6",
        "outputId": "d2daf912-cd94-47ef-88a1-d0bb03a8d197"
      },
      "source": [
        "token_df['token'][df['category'] == 'spam'].value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "call      198\n",
              "get       128\n",
              "know      107\n",
              "like       82\n",
              "got        81\n",
              "ltgt       79\n",
              "send       76\n",
              "free       76\n",
              "now        73\n",
              "going      64\n",
              "text       64\n",
              "you        62\n",
              "time       62\n",
              "need       61\n",
              "mobile     60\n",
              "good       55\n",
              "lor        54\n",
              "home       53\n",
              "day        52\n",
              "back       52\n",
              "Name: token, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYD2_PlmJbUm",
        "outputId": "cfe48a33-0da3-4a69-8b9f-1f659b2cc3c5"
      },
      "source": [
        "token_df['token'][df['category'] == 'ham'].value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "call     378\n",
              "get      258\n",
              "free     199\n",
              "ltgt     197\n",
              "good     179\n",
              "like     160\n",
              "now      158\n",
              "got      157\n",
              "day      150\n",
              "know     150\n",
              "love     147\n",
              "time     146\n",
              "ill      146\n",
              "you      145\n",
              "sorry    130\n",
              "text     124\n",
              "stop     118\n",
              "txt      117\n",
              "send     114\n",
              "home     109\n",
              "Name: token, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV5i6smMJvue",
        "outputId": "f9455304-b77e-4cfd-f9ab-27a6f2fcbdee"
      },
      "source": [
        "# drop common words seen in each category\n",
        "common_words = ['call', 'get', 'free', 'ltgt', 'text', 'you', 'got', 'like',\n",
        "                'send', 'now', 'txt', 'dont']\n",
        "token_df = token_df[token_df['token'].isin(common_words) == False]\n",
        "print(\"Drop common words across data categories: \" + str(len(token_df)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drop common words across data categories: 30783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTdn7pzKZzS",
        "outputId": "23752368-6f88-41e0-c6b6-9752cac5ec55"
      },
      "source": [
        "token_df['token'][df['category'] == 'spam'].value_counts().head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "know      107\n",
              "going      64\n",
              "time       62\n",
              "need       61\n",
              "mobile     60\n",
              "good       55\n",
              "lor        54\n",
              "home       53\n",
              "back       52\n",
              "day        52\n",
              "ill        51\n",
              "see        50\n",
              "still      49\n",
              "love       48\n",
              "today      47\n",
              "week       45\n",
              "right      45\n",
              "make       43\n",
              "way        43\n",
              "phone      42\n",
              "Name: token, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "lbjWEKYsWC4m",
        "outputId": "79d67ff0-9353-4c3b-a068-4adda4615b99"
      },
      "source": [
        "term_freq = (token_df\n",
        "              .groupby(['doc_id', 'token'], as_index = False)\n",
        "              .count())\n",
        "term_freq.columns = ['doc_id', 'term', 'term_count']\n",
        "term_freq.sort_values(by=['term_count'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>term</th>\n",
              "      <th>term_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11892</th>\n",
              "      <td>doc_3015</td>\n",
              "      <td>happy</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18059</th>\n",
              "      <td>doc_409</td>\n",
              "      <td>missing</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>doc_1139</td>\n",
              "      <td>missing</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11888</th>\n",
              "      <td>doc_3015</td>\n",
              "      <td>day</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3724</th>\n",
              "      <td>doc_1621</td>\n",
              "      <td>simple</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10034</th>\n",
              "      <td>doc_2700</td>\n",
              "      <td>frnds</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10033</th>\n",
              "      <td>doc_2700</td>\n",
              "      <td>feb</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10031</th>\n",
              "      <td>doc_2700</td>\n",
              "      <td>day</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10030</th>\n",
              "      <td>doc_2700</td>\n",
              "      <td>comes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29513</th>\n",
              "      <td>doc_999</td>\n",
              "      <td>thanks</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29514 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         doc_id     term  term_count\n",
              "11892  doc_3015    happy          15\n",
              "18059   doc_409  missing           6\n",
              "854    doc_1139  missing           6\n",
              "11888  doc_3015      day           6\n",
              "3724   doc_1621   simple           5\n",
              "...         ...      ...         ...\n",
              "10034  doc_2700    frnds           1\n",
              "10033  doc_2700      feb           1\n",
              "10031  doc_2700      day           1\n",
              "10030  doc_2700    comes           1\n",
              "29513   doc_999   thanks           1\n",
              "\n",
              "[29514 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "uEaiKWiveyEq",
        "outputId": "9fd07d9b-f2a3-43c0-d3ae-c2d9a120efd6"
      },
      "source": [
        "df[df['doc_id'] == 'doc_1139']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>doc_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>ham</td>\n",
              "      <td>Message:some text missing* Sender:Name Missing* *Number Missing *Sent:Date missing *Missing U a lot thats y everything is missing sent via fullonsms.com</td>\n",
              "      <td>doc_1139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     category  ...    doc_id\n",
              "1139      ham  ...  doc_1139\n",
              "\n",
              "[1 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TcMlHlBkQDp"
      },
      "source": [
        "# 3. Reshape the term frequency table to a document term matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "XSeUKHx4kAkY",
        "outputId": "739825eb-fb1c-455a-b7fc-2f0de8b24e47"
      },
      "source": [
        "doc_list = term_freq['doc_id'].unique().tolist()\n",
        "doc_list = doc_list[0:3] # minimal example to verify code is working\n",
        "\n",
        "reshape_dfs =[]\n",
        "for doc in doc_list:\n",
        "  temp = term_freq[['term_count']][term_freq['doc_id'] == doc].T\n",
        "  temp.columns = term_freq['term'][term_freq['doc_id'] == doc].tolist()\n",
        "  temp.index = [doc]\n",
        "  reshape_dfs.append(temp)\n",
        "\n",
        "doc_term_mat = pd.concat(reshape_dfs)\n",
        "doc_term_mat = doc_term_mat.fillna(0)\n",
        "doc_term_mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>available</th>\n",
              "      <th>bugis</th>\n",
              "      <th>crazy</th>\n",
              "      <th>great</th>\n",
              "      <th>point</th>\n",
              "      <th>wat</th>\n",
              "      <th>world</th>\n",
              "      <th>joking</th>\n",
              "      <th>lar</th>\n",
              "      <th>wif</th>\n",
              "      <th>anymore</th>\n",
              "      <th>enough</th>\n",
              "      <th>gonna</th>\n",
              "      <th>home</th>\n",
              "      <th>ive</th>\n",
              "      <th>soon</th>\n",
              "      <th>stuff</th>\n",
              "      <th>talk</th>\n",
              "      <th>today</th>\n",
              "      <th>tonight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc_0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doc_10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        available  bugis  crazy  great  ...  stuff  talk  today  tonight\n",
              "doc_0         1.0    1.0    1.0    1.0  ...    0.0   0.0    0.0      0.0\n",
              "doc_1         0.0    0.0    0.0    0.0  ...    0.0   0.0    0.0      0.0\n",
              "doc_10        0.0    0.0    0.0    0.0  ...    1.0   1.0    1.0      1.0\n",
              "\n",
              "[3 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQNR36RUnKKy",
        "outputId": "d4996a18-027f-4942-c86d-07624b467e0d"
      },
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "doc_list = term_freq['doc_id'].unique().tolist()\n",
        "reshape_dfs =[]\n",
        "counter = 0\n",
        "for doc in doc_list:\n",
        "  counter += 1\n",
        "  if counter % 500 == 0:\n",
        "    print(str(counter) + \" complete out of \" + str(len(doc_list)) + \" documents\")\n",
        "  temp = term_freq[['term_count']][term_freq['doc_id'] == doc].T\n",
        "  temp.columns = term_freq['term'][term_freq['doc_id'] == doc].tolist()\n",
        "  temp.index = [doc]\n",
        "  reshape_dfs.append(temp)\n",
        "\n",
        "print(\"\\n merging stuff\")\n",
        "start_time = time.time()\n",
        "doc_term_mat = pd.concat(reshape_dfs)\n",
        "doc_term_mat = doc_term_mat.fillna(0)\n",
        "\n",
        "print(\"--- %s time elapsed ---\" % str(timedelta(seconds=time.time() - start_time)))\n",
        "print(doc_term_mat.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 complete out of 5386 documents\n",
            "1000 complete out of 5386 documents\n",
            "1500 complete out of 5386 documents\n",
            "2000 complete out of 5386 documents\n",
            "2500 complete out of 5386 documents\n",
            "3000 complete out of 5386 documents\n",
            "3500 complete out of 5386 documents\n",
            "4000 complete out of 5386 documents\n",
            "4500 complete out of 5386 documents\n",
            "5000 complete out of 5386 documents\n",
            "\n",
            " merging stuff\n",
            "--- 0:06:29.792204 time elapsed ---\n",
            "(5386, 1591)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vbB3sjvDvf"
      },
      "source": [
        "# 4. Determine what terms matter for predicting spam\n",
        "\n",
        "Remove highly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pFALKjgvPn4"
      },
      "source": [
        "doc_term_mat2 = doc_term_mat\n",
        "doc_term_mat2['doc_id'] = doc_term_mat2.index\n",
        "doc_term_mat2 = doc_term_mat2.merge(df[['doc_id', 'category']], how = 'left', on = 'doc_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9btTwCU21IJ",
        "outputId": "d29d3fd1-46d4-48c0-dd6f-6e2564ef2817"
      },
      "source": [
        "def dichotomize_cat(txt):\n",
        "  x = 0\n",
        "  if txt == \"spam\":\n",
        "    x = 1\n",
        "  return x\n",
        "\n",
        "doc_term_mat2['category'] = doc_term_mat2['category'].apply(lambda x: dichotomize_cat(x))\n",
        "doc_term_mat2['category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4643\n",
              "1     743\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7EFD_oovamg"
      },
      "source": [
        "cols = list(doc_term_mat2)\n",
        "cols.insert(0, cols.pop(cols.index('doc_id')))\n",
        "cols.insert(1, cols.pop(cols.index('category')))\n",
        "doc_term_mat2 = doc_term_mat2[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJyF8pSSGINz"
      },
      "source": [
        "keep only significant variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhIj2suGzf1Y"
      },
      "source": [
        "Y = doc_term_mat2.loc[:, 'category']  # all rows of 'diagnosis' \n",
        "X = doc_term_mat2.drop(['doc_id', 'category'], axis=1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "X.insert(loc=len(X.columns), column='intercept', value=1) #### column of 1's\n",
        "X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xJQswG5C_Sb",
        "outputId": "c828dabb-fa6f-417d-f85f-86ec55ce2281"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "regression_ols = sm.OLS(y_train, X_train).fit()\n",
        "pvalues = regression_ols.pvalues \n",
        "sig_vars = []\n",
        "for i in range(0, len(pvalues)):\n",
        "  if pvalues[i] <= 0.5:\n",
        "    sig_vars.append(pvalues.index[i])\n",
        "print(len(sig_vars))\n",
        "sig_vars[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crazy',\n",
              " 'wat',\n",
              " 'lar',\n",
              " 'enough',\n",
              " 'gonna',\n",
              " 'home',\n",
              " 'ive',\n",
              " 'talk',\n",
              " 'today',\n",
              " 'long',\n",
              " 'pick',\n",
              " 'price',\n",
              " 'heard',\n",
              " 'tat',\n",
              " 'cash',\n",
              " 'guaranteed',\n",
              " 'please',\n",
              " 'prize',\n",
              " 'representative',\n",
              " 'service']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kSf0BNBHPaa",
        "outputId": "bad4f74c-bd2d-44bb-f40d-723a477b0567"
      },
      "source": [
        "X_train = X_train[sig_vars]\n",
        "X_test = X_test[sig_vars]\n",
        "\n",
        "regression_ols = sm.OLS(y_train, X_train).fit()\n",
        "pvalues = regression_ols.pvalues \n",
        "sig_vars = []\n",
        "for i in range(0, len(pvalues)):\n",
        "  if pvalues[i] <= 0.1:\n",
        "    sig_vars.append(pvalues.index[i])\n",
        "print(len(sig_vars))\n",
        "sig_vars[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crazy',\n",
              " 'enough',\n",
              " 'gonna',\n",
              " 'today',\n",
              " 'pick',\n",
              " 'price',\n",
              " 'heard',\n",
              " 'cash',\n",
              " 'guaranteed',\n",
              " 'please',\n",
              " 'prize',\n",
              " 'representative',\n",
              " 'service',\n",
              " 'â5000',\n",
              " 'told',\n",
              " 'give',\n",
              " '08000839402',\n",
              " 'mobileupd8',\n",
              " 'nokia',\n",
              " 'orange']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clT2Br4T9L9D"
      },
      "source": [
        "reg_strength = 1000\n",
        "\n",
        "### Formula in Slide 8\n",
        "def compute_cost(W, X, Y):\n",
        "  N = X.shape[0]\n",
        "  distances = 1 - Y * (np.dot(X, W))\n",
        "  distances[distances < 0] = 0\n",
        "  hinge_loss = reg_strength * (np.sum(distances) / N)\n",
        "  cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "  return(cost)\n",
        "\n",
        "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
        "    if type(Y_batch) != np.array: #float64:\n",
        "        Y_batch = np.array([Y_batch])\n",
        "        X_batch = np.array([X_batch])\n",
        "\n",
        "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
        "#    if type(distance)== np.float64:\n",
        "#      distance = np.array([distance])\n",
        "    dw = np.zeros(len(W))\n",
        "\n",
        "    #### for every dimension, apply formula in Slide 9\n",
        "    for ind, d in enumerate(distance):\n",
        "        if max(0, d) == 0:\n",
        "            di = W\n",
        "        else:\n",
        "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
        "        dw += di  \n",
        "    dw = dw/len(Y_batch)  # average\n",
        "    return dw\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "def sgd(features, outputs, max_epochs=5000, learning_rate=0.05,cost_threshold = 0.01):\n",
        "  weights = np.zeros(features.shape[1])\n",
        "  prev_cost = float(\"inf\")\n",
        "  nth = 0\n",
        "  for epoch in range(1, max_epochs):\n",
        "    X, Y = shuffle(features, outputs)\n",
        "    for ind, x in enumerate(X):  #### itereate through the dimensions\n",
        "      ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "      weights = weights - (learning_rate * ascent)\n",
        "\n",
        "    if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
        "      cost = compute_cost(weights, features, outputs)\n",
        "      print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
        "\n",
        "      #### If diffrence in the cost from last ^2 iterations to now is < 1%: get out\n",
        "      if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
        "            return weights\n",
        "      prev_cost = cost\n",
        "      nth += 1\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG_IDnc29Zgf",
        "outputId": "e7ce6994-65c0-453e-a710-39de663d726d"
      },
      "source": [
        "import numpy as np\n",
        "X_train = X_train[sig_vars]\n",
        "X_test = X_test[sig_vars]\n",
        "\n",
        "W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score \n",
        "def test():\n",
        "  y_test_predicted = np.array([])\n",
        "  for i in range(X_test.shape[0]):\n",
        "      yp = np.sign(np.dot(W, X_test.to_numpy()[i])) #model\n",
        "      y_test_predicted = np.append(y_test_predicted, yp)\n",
        "  print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n",
        "  print(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
        "  print(\"precision on test dataset: {}\".format(precision_score(y_test.to_numpy(), y_test_predicted)))\n",
        "\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch is:1 and Cost is: 910.1495237304057\n",
            "Epoch is:2 and Cost is: 872.1369238099027\n",
            "Epoch is:4 and Cost is: 961.898403646313\n",
            "Epoch is:8 and Cost is: 872.3843487732357\n",
            "Epoch is:16 and Cost is: 1008.4441357908554\n",
            "Epoch is:32 and Cost is: 2026.9460913385926\n",
            "Epoch is:64 and Cost is: 931.1829656458793\n",
            "Epoch is:128 and Cost is: 4352.157332200734\n",
            "Epoch is:256 and Cost is: 5344.417652885657\n",
            "Epoch is:512 and Cost is: 1145.647817346647\n",
            "Epoch is:1024 and Cost is: 866.1646721692231\n",
            "Epoch is:2048 and Cost is: 994.1051022919514\n",
            "Epoch is:4096 and Cost is: 1240.126225004275\n",
            "Epoch is:4999 and Cost is: 2841.85000390915\n",
            "accuracy on test dataset: 0.14285714285714285\n",
            "recall on test dataset: 1.0\n",
            "precision on test dataset: 0.14285714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRD5mUjGWd6f"
      },
      "source": [
        "^^ This is a very terrible model.  14% precision means we incorrectly predict spam all the time. Now we will try the sklearn version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdnjzL7tWtI9"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from statistics import mean\n",
        "from numpy import std\n",
        "\n",
        "def eval_model(model, print_text, X, y):\n",
        "  cv = KFold(n_splits=10) \n",
        "  accuracy = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv)\n",
        "  accuracy = list(accuracy)\n",
        "  f1_scores = cross_val_score(model, X, y, scoring = 'f1', cv = cv)\n",
        "  f1_scores = list(f1_scores)\n",
        "  precision_scores = cross_val_score(model, X, y, scoring = 'precision', cv = cv)\n",
        "  precision_scores = list(precision_scores)\n",
        "  recall_scores = cross_val_score(model, X, y, scoring = 'recall', cv = cv)\n",
        "  recall_scores = list(recall_scores)\n",
        "\n",
        "  print(print_text)\n",
        "  print('accuracy score: ' + str(mean(accuracy))[0:5] + \" +/- \" + str(std(accuracy))[0:5])\n",
        "  print('f1 score: ' + str(mean(f1_scores))[0:5] + \" +/- \" + str(std(f1_scores))[0:5])\n",
        "  print('precision: ' + str(mean(precision_scores))[0:5] + \" +/- \" + str(std(precision_scores))[0:5])\n",
        "  print('recall: '+ str(mean(recall_scores))[0:5] + \" +/- \" + str(std(recall_scores))[0:5] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajmu2TjwXCF_",
        "outputId": "4585f9e3-4c16-405e-8c57-9457307631af"
      },
      "source": [
        "eval_model(model = GaussianNB(), X = X_train, y = y_train, print_text=\"Naive Bayes\")\n",
        "eval_model(model = SVC(), X = X_train, y = y_train, print_text=\"Support Vector Machine\")\n",
        "eval_model(model = RandomForestClassifier(), X = X_train, y = y_train, print_text=\"Random Forest\")\n",
        "eval_model(model = KNeighborsClassifier(), X = X_train, y = y_train, print_text=\"KNN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes\n",
            "accuracy score: 0.965 +/- 0.006\n",
            "f1 score: 0.876 +/- 0.024\n",
            "precision: 0.851 +/- 0.036\n",
            "recall: 0.905 +/- 0.044\n",
            "\n",
            "Support Vector Machine\n",
            "accuracy score: 0.972 +/- 0.007\n",
            "f1 score: 0.896 +/- 0.029\n",
            "precision: 0.923 +/- 0.039\n",
            "recall: 0.871 +/- 0.039\n",
            "\n",
            "Random Forest\n",
            "accuracy score: 0.972 +/- 0.005\n",
            "f1 score: 0.889 +/- 0.025\n",
            "precision: 0.965 +/- 0.021\n",
            "recall: 0.820 +/- 0.051\n",
            "\n",
            "KNN\n",
            "accuracy score: 0.920 +/- 0.009\n",
            "f1 score: 0.591 +/- 0.057\n",
            "precision: 0.996 +/- 0.012\n",
            "recall: 0.423 +/- 0.057\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxdAZgMaHFhK"
      },
      "source": [
        "## Stuff that didn't work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEC_pLWiyFKf"
      },
      "source": [
        "### Remove duplicate / correlated features\n",
        "def remove_correlated_features(X, corr_threshold=0.9):\n",
        "  corr = X.corr()\n",
        "  drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
        "  for i in range(corr.shape[0]):\n",
        "      for j in range(i + 1, corr.shape[0]):\n",
        "          if corr.iloc[i, j] >= corr_threshold:\n",
        "              drop_columns[j] = True\n",
        "  columns_dropped = X.columns[drop_columns]\n",
        "  print(\"dropping\",columns_dropped)\n",
        "  X_drop = X.drop(columns_dropped, axis=1)\n",
        "  return X_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_PPEoJcyOgq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = doc_term_mat2.drop(['doc_id', 'category'], axis=1)\n",
        "X_drop = remove_correlated_features(X, corr_threshold=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIdHjtV8z7Jg"
      },
      "source": [
        "Remove less significant features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYSCKKUP4dM0"
      },
      "source": [
        "#regression_ols = sm.OLS(y_train, X_train).fit()\n",
        "#regression_ols.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpbvqDMB0HWX"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "def remove_less_significant_features(X, Y, sl=0.05):\n",
        "  regression_ols = None\n",
        "  columns_dropped = np.array([])\n",
        "  #for itr in range(0, len(X.columns)):\n",
        "  for itr in range(0, 60):\n",
        "    if itr % 5 == 0:\n",
        "      print(\"Iteration number \" + str(itr))\n",
        "    regression_ols = sm.OLS(Y, X).fit()\n",
        "    max_col = regression_ols.pvalues.idxmax()\n",
        "    max_val = regression_ols.pvalues.max()\n",
        "    if max_val > sl:\n",
        "        X.drop(max_col, axis='columns', inplace=True)\n",
        "        columns_dropped = np.append(columns_dropped, [max_col])\n",
        "    else:\n",
        "        break\n",
        "  regression_ols.summary()\n",
        "  return columns_dropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Hg7yDY0mB-"
      },
      "source": [
        "start_time = time.time()\n",
        "cols_dropped =  remove_less_significant_features(X_train,y_train, sl=0.05)\n",
        "print(\"--- %s time elapsed ---\" % str(timedelta(seconds=time.time() - start_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UGiBy-YAsO1"
      },
      "source": [
        "cols_dropped"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}